{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sO_XwDIDt8Yp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Define the hyperparameters\n",
        "vocab_size = 10000  # The size of the vocabulary\n",
        "max_length = 200  # The maximum length of a sentence\n",
        "embedding_dim = 32  # The dimensions of the word embedding\n",
        "num_filters = 128  # The number of filters for the convolutional layer\n",
        "kernel_size = 3  # The kernel size for the convolutional layer\n",
        "hidden_dims = 64  # The number of units in the hidden dense layer\n",
        "batch_size = 128  # The batch size\n",
        "epochs = 10  # The number of epochs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BSSi2hq7xNHg"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q2hCzfJV56aD"
      },
      "outputs": [],
      "source": [
        "def sequences_to_matrix(sequences, mode='binary'):\n",
        "  # create a matrix with the size of the vocabulary\n",
        "  # vocab_size = len(set([item for sublist in sequences for item in sublist]))\n",
        "  matrix = np.zeros((len(sequences), vocab_size))\n",
        "  # fill the matrix according to the mode\n",
        "  if mode == 'binary':\n",
        "      for i, seq in enumerate(sequences):\n",
        "          matrix[i, seq] = 1\n",
        "  elif mode == 'count':\n",
        "      for i, seq in enumerate(sequences):\n",
        "          matrix[i, seq] += 1\n",
        "  elif mode == 'tfidf':\n",
        "      # calculate the document frequency of each word\n",
        "      df = np.zeros(vocab_size)\n",
        "      for seq in sequences:\n",
        "          df[seq] += 1\n",
        "      # calculate the inverse document frequency\n",
        "      idf = np.log((1 + len(sequences)) / (1 + df)) + 1\n",
        "      # calculate the term frequency\n",
        "      for i, seq in enumerate(sequences):\n",
        "          matrix[i, seq] = np.log(1 + seq.count(seq)) * idf[seq]\n",
        "\n",
        "  return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITlQzhVS8eD1"
      },
      "outputs": [],
      "source": [
        "def pad_sequences(sequences, maxlen=None, padding='pre', truncating='pre'):\n",
        "  # get the maximum length of the sequences\n",
        "  if maxlen is None:\n",
        "    maxlen = max([len(seq) for seq in sequences])\n",
        "  \n",
        "  # create a matrix with the size of the padded sequences\n",
        "  matrix = np.zeros((len(sequences), maxlen))\n",
        "\n",
        "  # pad the sequences\n",
        "  for i, seq in enumerate(sequences):\n",
        "      if padding == 'pre':\n",
        "          matrix[i, -len(seq):] = seq[:maxlen]\n",
        "      elif padding == 'post':\n",
        "          matrix[i, :len(seq)] = seq[-maxlen:]\n",
        "\n",
        "  # truncate the sequences\n",
        "  if truncating == 'pre':\n",
        "      matrix = matrix[:, -maxlen:]\n",
        "  elif truncating == 'post':\n",
        "      matrix = matrix[:, :maxlen]\n",
        "\n",
        "  return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jDWcJhXxodF"
      },
      "outputs": [],
      "source": [
        "# Convert the data to sequences of integers\n",
        "x_train = sequences_to_matrix(x_train, mode=\"binary\")\n",
        "x_test = sequences_to_matrix(x_test, mode=\"binary\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad4arJxtBVNr",
        "outputId": "b17fd7a0-b218-4898-b068-8cedf3a28b83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(x_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUwSB_iuxq9A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Noux5-UFxqKo"
      },
      "outputs": [],
      "source": [
        "# Pad the sequences to the same length\n",
        "x_train = pad_sequences(x_train, maxlen=max_length, padding=\"post\", truncating=\"pre\")\n",
        "x_test = pad_sequences(x_test, maxlen=max_length, padding=\"post\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_aCbXIHznAO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RT0Wnf6xkBb",
        "outputId": "f3173c25-fff0-47a8-c7a0-12c0e02ff8d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "196/196 [==============================] - 19s 93ms/step - loss: 0.6145 - accuracy: 0.6400\n",
            "Epoch 2/10\n",
            "196/196 [==============================] - 19s 97ms/step - loss: 0.5621 - accuracy: 0.6984\n",
            "Epoch 3/10\n",
            "196/196 [==============================] - 19s 99ms/step - loss: 0.5516 - accuracy: 0.7068\n",
            "Epoch 4/10\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 0.5425 - accuracy: 0.7106\n",
            "Epoch 5/10\n",
            "196/196 [==============================] - 18s 92ms/step - loss: 0.5336 - accuracy: 0.7189\n",
            "Epoch 6/10\n",
            "196/196 [==============================] - 21s 107ms/step - loss: 0.5254 - accuracy: 0.7257\n",
            "Epoch 7/10\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 0.5191 - accuracy: 0.7302\n",
            "Epoch 8/10\n",
            "196/196 [==============================] - 18s 92ms/step - loss: 0.5048 - accuracy: 0.7406\n",
            "Epoch 9/10\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 0.4920 - accuracy: 0.7491\n",
            "Epoch 10/10\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 0.4757 - accuracy: 0.7630\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.6025 - accuracy: 0.6880\n",
            "Test loss: 0.603\n",
            "Test accuracy: 0.688\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Add the embedding layer\n",
        "model.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "\n",
        "# Add the convolutional and pooling layers\n",
        "model.add(tf.keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
        "model.add(tf.keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
        "model.add(tf.keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Add the dense layers\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(units=hidden_dims, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(units=hidden_dims, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "score = model.evaluate(x_test, y_test)\n",
        "print(\"Test loss: {:.3f}\".format(score[0]))\n",
        "print(\"Test accuracy: {:.3f}\".format(score[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzmS2qVGxhen"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}